{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "affect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih3SbHcJmD3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow\n",
        "from PIL import ImageFile, Image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import glob\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, BatchNormalization, LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oynzmja2nb2Y",
        "colab_type": "code",
        "outputId": "b543683d-c34e-4b9d-bc04-73c0647a2360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYCPxpVRsKmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Размеры изображения\n",
        "img_width, img_height = 150, 150\n",
        "# Размерность тензора на основе изображения для входных данных в нейронную сеть\n",
        "# backend Tensorflow, channels_last\n",
        "input_shape = (img_width, img_height, 3)\n",
        "# Количество эпох\n",
        "epochs = 10\n",
        "# Размер мини-выборки\n",
        "batch_size = 64\n",
        "# Количество изображений для обучения\n",
        "nb_train_samples = 13650\n",
        "# Количество изображений для проверки\n",
        "nb_validation_samples = 2925\n",
        "# Количество изображений для тестирования\n",
        "nb_test_samples = 2925\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Xh_v1xrI8N",
        "colab_type": "code",
        "outputId": "4a9dea79-c7a7-487b-9161-71ba136fd6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = []\n",
        "filenames = []\n",
        "c = 0\n",
        "for filename in sorted(glob.glob('/content/drive/My Drive/train/*.jpg')):\n",
        "  image = cv2.imread(filename)\n",
        "  image = cv2.resize(image, (img_width, img_height))\n",
        "  filenames.append(filename)\n",
        "  X.append(image)\n",
        "  c += 1\n",
        "  print(c)\n",
        "print(X[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyYRTZxRueHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.divide(X, 255.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGQnGTNLnkec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = pd.read_csv('/content/drive/My Drive/training.csv', sep=',')\n",
        "Y = Y[['subDirectory_filePath', 'valence', 'arousal']]\n",
        "Y['subDirectory_filePath'] = Y['subDirectory_filePath'].apply(lambda x: x.split(\"/\")[1])\n",
        "Y = Y.sort_values('subDirectory_filePath')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xo-9Z1WFqj1",
        "colab_type": "code",
        "outputId": "91dfae62-4464-41bc-cb05-de78469e8327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "Y.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subDirectory_filePath</th>\n",
              "      <th>valence</th>\n",
              "      <th>arousal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34738</th>\n",
              "      <td>0000792211b64f6a59bd2d95fee49eabe6373ec1d88f22...</td>\n",
              "      <td>-0.488145</td>\n",
              "      <td>0.821877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157036</th>\n",
              "      <td>0000f8a4575c15055a9ee0a72c9aa5bf9ac00558173565...</td>\n",
              "      <td>-0.316056</td>\n",
              "      <td>-0.136957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26740</th>\n",
              "      <td>00012312112a15995f1d1c1ac640db7191eacab8099e90...</td>\n",
              "      <td>0.710109</td>\n",
              "      <td>0.689223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178824</th>\n",
              "      <td>0001636b7a16a63b2d9c8f5e4b7be02e4841d3e0af3ebd...</td>\n",
              "      <td>-0.420635</td>\n",
              "      <td>0.515873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298470</th>\n",
              "      <td>0001720743cf22095bd3b2c94b35f244faf47545e26168...</td>\n",
              "      <td>0.521274</td>\n",
              "      <td>0.126796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    subDirectory_filePath   valence   arousal\n",
              "34738   0000792211b64f6a59bd2d95fee49eabe6373ec1d88f22... -0.488145  0.821877\n",
              "157036  0000f8a4575c15055a9ee0a72c9aa5bf9ac00558173565... -0.316056 -0.136957\n",
              "26740   00012312112a15995f1d1c1ac640db7191eacab8099e90...  0.710109  0.689223\n",
              "178824  0001636b7a16a63b2d9c8f5e4b7be02e4841d3e0af3ebd... -0.420635  0.515873\n",
              "298470  0001720743cf22095bd3b2c94b35f244faf47545e26168...  0.521274  0.126796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOWVRwtBvCVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y1 = []\n",
        "# for index, row in Y.iterrows():\n",
        "#   for f in filenames:\n",
        "#     if f.find(row['subDirectory_filePath']) >= 0:\n",
        "#       Y1.append(row)\n",
        "#       print(\"appending \" + str(row))\n",
        "#       break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNjFsqGyiikz",
        "colab_type": "code",
        "outputId": "97cd5f0e-e2b6-4e7f-c70a-00588827ca6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/y_modified.csv', sep=',')\n",
        "df = df[['subDirectory_filePath', 'valence', 'arousal']]\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subDirectory_filePath</th>\n",
              "      <th>valence</th>\n",
              "      <th>arousal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000c10a8f6735212d71eb54298bc47ea56dc08e64ed2ca...</td>\n",
              "      <td>0.004313</td>\n",
              "      <td>-0.008627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>001fffed9707757082906ff2eb89cad6ddf242195fe6a5...</td>\n",
              "      <td>0.539683</td>\n",
              "      <td>0.023810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>002b6db57713be4a264d8dd13ee543b031de7577af983c...</td>\n",
              "      <td>-0.193573</td>\n",
              "      <td>0.590399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003e5febbe7808b02f6cafe17878cdfd59851726451209...</td>\n",
              "      <td>0.526805</td>\n",
              "      <td>-0.036153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>004b10f6bfbce7f58d124ac653451bdd975a7113d20b18...</td>\n",
              "      <td>0.849206</td>\n",
              "      <td>0.238095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>7a2b38adf6aa2d7b8b93d02f5defb7a6aebd738f8e544e...</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>0.063492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>7a2e5d157c9cbe119103f1a60052350ef3cbd14f651628...</td>\n",
              "      <td>-0.111111</td>\n",
              "      <td>-0.920635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>7a7b4f6fb23aaa9189a6faf644d07780947b00048d6992...</td>\n",
              "      <td>0.552105</td>\n",
              "      <td>0.219979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2140</th>\n",
              "      <td>7a7ea6d03d1cf4275e6dfc6d26104467c079edccecfcb0...</td>\n",
              "      <td>0.711698</td>\n",
              "      <td>0.125086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2141</th>\n",
              "      <td>7a8a674ef4503df80778541cf1c92a91c67f5bfbd9f364...</td>\n",
              "      <td>0.635431</td>\n",
              "      <td>0.049763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2142 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  subDirectory_filePath   valence   arousal\n",
              "0     000c10a8f6735212d71eb54298bc47ea56dc08e64ed2ca...  0.004313 -0.008627\n",
              "1     001fffed9707757082906ff2eb89cad6ddf242195fe6a5...  0.539683  0.023810\n",
              "2     002b6db57713be4a264d8dd13ee543b031de7577af983c... -0.193573  0.590399\n",
              "3     003e5febbe7808b02f6cafe17878cdfd59851726451209...  0.526805 -0.036153\n",
              "4     004b10f6bfbce7f58d124ac653451bdd975a7113d20b18...  0.849206  0.238095\n",
              "...                                                 ...       ...       ...\n",
              "2137  7a2b38adf6aa2d7b8b93d02f5defb7a6aebd738f8e544e...  0.380952  0.063492\n",
              "2138  7a2e5d157c9cbe119103f1a60052350ef3cbd14f651628... -0.111111 -0.920635\n",
              "2139  7a7b4f6fb23aaa9189a6faf644d07780947b00048d6992...  0.552105  0.219979\n",
              "2140  7a7ea6d03d1cf4275e6dfc6d26104467c079edccecfcb0...  0.711698  0.125086\n",
              "2141  7a8a674ef4503df80778541cf1c92a91c67f5bfbd9f364...  0.635431  0.049763\n",
              "\n",
              "[2142 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2mhJL34IT4",
        "colab_type": "code",
        "outputId": "0eec2980-5e7f-44ed-867f-8c3684e336e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "d = dict(zip(filenames, X))\n",
        "list(d.keys())[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/train/000c10a8f6735212d71eb54298bc47ea56dc08e64ed2cab8913d683f.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBPtOrbVgoUD",
        "colab_type": "code",
        "outputId": "158772b8-295f-4772-e4ea-3b5cf84ecba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "files = df['subDirectory_filePath']\n",
        "new_dict = {}\n",
        "for key in d.keys():\n",
        "  for file in files:\n",
        "    if key.find(file) >= 0:\n",
        "      new_dict.update({key:d[key]})\n",
        "      break\n",
        "print(len(new_dict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHI2WOXltMT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['image'] = list(new_dict.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTnWoxrJuvpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexes_to_drop = df[(df['valence'] == -2) | (df['arousal'] == -2)].index\n",
        "df.drop(indexes_to_drop, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7U-Yn4WmlgT",
        "colab_type": "code",
        "outputId": "18caa3a4-fb78-4f2d-fa5b-7e9beeb7f02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model for valence\n",
        "model_v = Sequential()\n",
        "model_v.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
        "model_v.add(Activation('relu'))\n",
        "model_v.add(BatchNormalization(axis=-1))\n",
        "model_v.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_v.add(Conv2D(32, (3, 3)))\n",
        "model_v.add(Activation('relu'))\n",
        "model_v.add(BatchNormalization(axis=-1))\n",
        "model_v.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_v.add(Conv2D(64, (3, 3)))\n",
        "model_v.add(Activation('relu'))\n",
        "model_v.add(BatchNormalization(axis=-1))\n",
        "model_v.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_v.add(Conv2D(64, (3, 3)))\n",
        "model_v.add(Activation('relu'))\n",
        "model_v.add(BatchNormalization(axis=-1))\n",
        "model_v.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_v.add(Conv2D(128, (3, 3)))\n",
        "model_v.add(Activation('relu'))\n",
        "model_v.add(BatchNormalization(axis=-1))\n",
        "model_v.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_v.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model_v.add(Dense(100))\n",
        "model_v.add(Activation('relu'))\n",
        "model_v.add(BatchNormalization(axis=-1))\n",
        "model_v.add(Dropout(0.5))\n",
        "model_v.add(Dense(1, activation='linear'))\n",
        "\n",
        "model_v.summary()\n",
        "\n",
        "model_v.compile(loss='mean_squared_error', optimizer='sgd')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 148, 148, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 148, 148, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 72, 72, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 72, 72, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 72, 72, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 36, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 34, 34, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 34, 34, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 15, 15, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 5, 5, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               51300     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 192,505\n",
            "Trainable params: 191,665\n",
            "Non-trainable params: 840\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRmKIPFhXrK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model for arousal\n",
        "model_a = Sequential()\n",
        "model_a.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
        "model_a.add(Activation('relu'))\n",
        "model_a.add(BatchNormalization(axis=-1))\n",
        "model_a.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_a.add(Conv2D(32, (3, 3)))\n",
        "model_a.add(Activation('relu'))\n",
        "model_a.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_a.add(Conv2D(64, (3, 3)))\n",
        "model_a.add(Activation('relu'))\n",
        "model_a.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "\n",
        "model_a.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model_a.add(Dense(50))\n",
        "model_a.add(Activation('relu'))\n",
        "model_a.add(BatchNormalization(axis=-1))\n",
        "model_a.add(Dropout(0.5))\n",
        "model_a.add(Dense(4, activation='relu'))\n",
        "model_a.add(Dense(1, activation='linear'))\n",
        "\n",
        "model_a.summary()\n",
        "\n",
        "model_a.compile(loss='mean_squared_error', optimizer='sgd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAPzVZfWD97X",
        "colab_type": "code",
        "outputId": "c6968f92-2a5c-43ef-b053-b4ccd2389672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_train_v, x_test_v, y_train_v, y_test_v = train_test_split(np.stack(list(df['image']), axis=0), list(df['valence']), test_size=0.33)\n",
        "\n",
        "model_v.fit(x_train_v, y_train_v, epochs=200, batch_size=64, validation_data=(x_test_v, y_test_v))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 1179 samples, validate on 581 samples\n",
            "Epoch 1/200\n",
            "1179/1179 [==============================] - 9s 7ms/step - loss: 2.2422 - val_loss: 0.7328\n",
            "Epoch 2/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.6132 - val_loss: 0.2945\n",
            "Epoch 3/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.3790 - val_loss: 0.2519\n",
            "Epoch 4/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.2878 - val_loss: 0.2404\n",
            "Epoch 5/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.2414 - val_loss: 0.2304\n",
            "Epoch 6/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.2171 - val_loss: 0.2354\n",
            "Epoch 7/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.2156 - val_loss: 0.2295\n",
            "Epoch 8/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.2073 - val_loss: 0.2288\n",
            "Epoch 9/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.2181 - val_loss: 0.2276\n",
            "Epoch 10/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.2031 - val_loss: 0.2307\n",
            "Epoch 11/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.2002 - val_loss: 0.2332\n",
            "Epoch 12/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.1896 - val_loss: 0.2282\n",
            "Epoch 13/200\n",
            "1179/1179 [==============================] - 1s 1000us/step - loss: 0.1927 - val_loss: 0.2273\n",
            "Epoch 14/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1840 - val_loss: 0.2276\n",
            "Epoch 15/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1753 - val_loss: 0.2293\n",
            "Epoch 16/200\n",
            "1179/1179 [==============================] - 1s 998us/step - loss: 0.1729 - val_loss: 0.2263\n",
            "Epoch 17/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.1691 - val_loss: 0.2244\n",
            "Epoch 18/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.1718 - val_loss: 0.2258\n",
            "Epoch 19/200\n",
            "1179/1179 [==============================] - 1s 988us/step - loss: 0.1562 - val_loss: 0.2254\n",
            "Epoch 20/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1591 - val_loss: 0.2288\n",
            "Epoch 21/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1448 - val_loss: 0.2287\n",
            "Epoch 22/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1527 - val_loss: 0.2254\n",
            "Epoch 23/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.1508 - val_loss: 0.2244\n",
            "Epoch 24/200\n",
            "1179/1179 [==============================] - 1s 976us/step - loss: 0.1374 - val_loss: 0.2252\n",
            "Epoch 25/200\n",
            "1179/1179 [==============================] - 1s 988us/step - loss: 0.1334 - val_loss: 0.2284\n",
            "Epoch 26/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1361 - val_loss: 0.2248\n",
            "Epoch 27/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1198 - val_loss: 0.2289\n",
            "Epoch 28/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.1269 - val_loss: 0.2295\n",
            "Epoch 29/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1170 - val_loss: 0.2294\n",
            "Epoch 30/200\n",
            "1179/1179 [==============================] - 1s 998us/step - loss: 0.1096 - val_loss: 0.2260\n",
            "Epoch 31/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1186 - val_loss: 0.2256\n",
            "Epoch 32/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.1183 - val_loss: 0.2253\n",
            "Epoch 33/200\n",
            "1179/1179 [==============================] - 1s 990us/step - loss: 0.1080 - val_loss: 0.2294\n",
            "Epoch 34/200\n",
            "1179/1179 [==============================] - 1s 990us/step - loss: 0.1137 - val_loss: 0.2280\n",
            "Epoch 35/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0998 - val_loss: 0.2303\n",
            "Epoch 36/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0999 - val_loss: 0.2308\n",
            "Epoch 37/200\n",
            "1179/1179 [==============================] - 1s 995us/step - loss: 0.1009 - val_loss: 0.2249\n",
            "Epoch 38/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1021 - val_loss: 0.2263\n",
            "Epoch 39/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.1000 - val_loss: 0.2316\n",
            "Epoch 40/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.1011 - val_loss: 0.2235\n",
            "Epoch 41/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0951 - val_loss: 0.2317\n",
            "Epoch 42/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0880 - val_loss: 0.2275\n",
            "Epoch 43/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0860 - val_loss: 0.2309\n",
            "Epoch 44/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0901 - val_loss: 0.2362\n",
            "Epoch 45/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0872 - val_loss: 0.2277\n",
            "Epoch 46/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0839 - val_loss: 0.2205\n",
            "Epoch 47/200\n",
            "1179/1179 [==============================] - 1s 986us/step - loss: 0.0813 - val_loss: 0.2198\n",
            "Epoch 48/200\n",
            "1179/1179 [==============================] - 1s 983us/step - loss: 0.0818 - val_loss: 0.2229\n",
            "Epoch 49/200\n",
            "1179/1179 [==============================] - 1s 977us/step - loss: 0.0809 - val_loss: 0.2261\n",
            "Epoch 50/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0758 - val_loss: 0.2244\n",
            "Epoch 51/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0757 - val_loss: 0.2286\n",
            "Epoch 52/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0812 - val_loss: 0.2288\n",
            "Epoch 53/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0781 - val_loss: 0.2251\n",
            "Epoch 54/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0701 - val_loss: 0.2278\n",
            "Epoch 55/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0718 - val_loss: 0.2281\n",
            "Epoch 56/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0709 - val_loss: 0.2279\n",
            "Epoch 57/200\n",
            "1179/1179 [==============================] - 1s 998us/step - loss: 0.0723 - val_loss: 0.2275\n",
            "Epoch 58/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0673 - val_loss: 0.2285\n",
            "Epoch 59/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0707 - val_loss: 0.2316\n",
            "Epoch 60/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0689 - val_loss: 0.2275\n",
            "Epoch 61/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0729 - val_loss: 0.2322\n",
            "Epoch 62/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0659 - val_loss: 0.2263\n",
            "Epoch 63/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0676 - val_loss: 0.2270\n",
            "Epoch 64/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0627 - val_loss: 0.2250\n",
            "Epoch 65/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0629 - val_loss: 0.2488\n",
            "Epoch 66/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0600 - val_loss: 0.2245\n",
            "Epoch 67/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0573 - val_loss: 0.2272\n",
            "Epoch 68/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0672 - val_loss: 0.2261\n",
            "Epoch 69/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0602 - val_loss: 0.2209\n",
            "Epoch 70/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0591 - val_loss: 0.2272\n",
            "Epoch 71/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0578 - val_loss: 0.2218\n",
            "Epoch 72/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0616 - val_loss: 0.2233\n",
            "Epoch 73/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0544 - val_loss: 0.2252\n",
            "Epoch 74/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0594 - val_loss: 0.2229\n",
            "Epoch 75/200\n",
            "1179/1179 [==============================] - 1s 974us/step - loss: 0.0531 - val_loss: 0.2207\n",
            "Epoch 76/200\n",
            "1179/1179 [==============================] - 1s 990us/step - loss: 0.0554 - val_loss: 0.2246\n",
            "Epoch 77/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0496 - val_loss: 0.2310\n",
            "Epoch 78/200\n",
            "1179/1179 [==============================] - 1s 998us/step - loss: 0.0521 - val_loss: 0.2227\n",
            "Epoch 79/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0540 - val_loss: 0.2270\n",
            "Epoch 80/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0525 - val_loss: 0.2248\n",
            "Epoch 81/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0550 - val_loss: 0.2252\n",
            "Epoch 82/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0526 - val_loss: 0.2222\n",
            "Epoch 83/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0496 - val_loss: 0.2427\n",
            "Epoch 84/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0542 - val_loss: 0.2285\n",
            "Epoch 85/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0560 - val_loss: 0.2247\n",
            "Epoch 86/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0504 - val_loss: 0.2291\n",
            "Epoch 87/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0519 - val_loss: 0.2225\n",
            "Epoch 88/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0474 - val_loss: 0.2252\n",
            "Epoch 89/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0468 - val_loss: 0.2245\n",
            "Epoch 90/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0482 - val_loss: 0.2167\n",
            "Epoch 91/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0515 - val_loss: 0.2252\n",
            "Epoch 92/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0472 - val_loss: 0.2207\n",
            "Epoch 93/200\n",
            "1179/1179 [==============================] - 1s 988us/step - loss: 0.0426 - val_loss: 0.2188\n",
            "Epoch 94/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0451 - val_loss: 0.2243\n",
            "Epoch 95/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0462 - val_loss: 0.2209\n",
            "Epoch 96/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0456 - val_loss: 0.2230\n",
            "Epoch 97/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0471 - val_loss: 0.2260\n",
            "Epoch 98/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0482 - val_loss: 0.2242\n",
            "Epoch 99/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0426 - val_loss: 0.2239\n",
            "Epoch 100/200\n",
            "1179/1179 [==============================] - 1s 982us/step - loss: 0.0485 - val_loss: 0.2210\n",
            "Epoch 101/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0440 - val_loss: 0.2230\n",
            "Epoch 102/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0405 - val_loss: 0.2216\n",
            "Epoch 103/200\n",
            "1179/1179 [==============================] - 1s 990us/step - loss: 0.0411 - val_loss: 0.2210\n",
            "Epoch 104/200\n",
            "1179/1179 [==============================] - 1s 998us/step - loss: 0.0439 - val_loss: 0.2204\n",
            "Epoch 105/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0419 - val_loss: 0.2171\n",
            "Epoch 106/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0451 - val_loss: 0.2211\n",
            "Epoch 107/200\n",
            "1179/1179 [==============================] - 1s 1000us/step - loss: 0.0468 - val_loss: 0.2191\n",
            "Epoch 108/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0434 - val_loss: 0.2238\n",
            "Epoch 109/200\n",
            "1179/1179 [==============================] - 1s 995us/step - loss: 0.0440 - val_loss: 0.2190\n",
            "Epoch 110/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0418 - val_loss: 0.2167\n",
            "Epoch 111/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0376 - val_loss: 0.2177\n",
            "Epoch 112/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0379 - val_loss: 0.2302\n",
            "Epoch 113/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0410 - val_loss: 0.2292\n",
            "Epoch 114/200\n",
            "1179/1179 [==============================] - 1s 995us/step - loss: 0.0397 - val_loss: 0.2222\n",
            "Epoch 115/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0421 - val_loss: 0.2178\n",
            "Epoch 116/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0407 - val_loss: 0.2267\n",
            "Epoch 117/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0385 - val_loss: 0.2181\n",
            "Epoch 118/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0406 - val_loss: 0.2170\n",
            "Epoch 119/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0397 - val_loss: 0.2182\n",
            "Epoch 120/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0405 - val_loss: 0.2220\n",
            "Epoch 121/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0369 - val_loss: 0.2168\n",
            "Epoch 122/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0362 - val_loss: 0.2237\n",
            "Epoch 123/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0405 - val_loss: 0.2209\n",
            "Epoch 124/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0408 - val_loss: 0.2208\n",
            "Epoch 125/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0336 - val_loss: 0.2164\n",
            "Epoch 126/200\n",
            "1179/1179 [==============================] - 1s 980us/step - loss: 0.0379 - val_loss: 0.2210\n",
            "Epoch 127/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0375 - val_loss: 0.2165\n",
            "Epoch 128/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0355 - val_loss: 0.2163\n",
            "Epoch 129/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0356 - val_loss: 0.2156\n",
            "Epoch 130/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0359 - val_loss: 0.2234\n",
            "Epoch 131/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0344 - val_loss: 0.2314\n",
            "Epoch 132/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0378 - val_loss: 0.2160\n",
            "Epoch 133/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0361 - val_loss: 0.2203\n",
            "Epoch 134/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0390 - val_loss: 0.2241\n",
            "Epoch 135/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0340 - val_loss: 0.2200\n",
            "Epoch 136/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0385 - val_loss: 0.2255\n",
            "Epoch 137/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0353 - val_loss: 0.2207\n",
            "Epoch 138/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0358 - val_loss: 0.2212\n",
            "Epoch 139/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0357 - val_loss: 0.2153\n",
            "Epoch 140/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0360 - val_loss: 0.2143\n",
            "Epoch 141/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0362 - val_loss: 0.2151\n",
            "Epoch 142/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0350 - val_loss: 0.2131\n",
            "Epoch 143/200\n",
            "1179/1179 [==============================] - 1s 998us/step - loss: 0.0341 - val_loss: 0.2207\n",
            "Epoch 144/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0334 - val_loss: 0.2180\n",
            "Epoch 145/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0316 - val_loss: 0.2099\n",
            "Epoch 146/200\n",
            "1179/1179 [==============================] - 1s 990us/step - loss: 0.0332 - val_loss: 0.2177\n",
            "Epoch 147/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0369 - val_loss: 0.2140\n",
            "Epoch 148/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0327 - val_loss: 0.2185\n",
            "Epoch 149/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0355 - val_loss: 0.2158\n",
            "Epoch 150/200\n",
            "1179/1179 [==============================] - 1s 981us/step - loss: 0.0319 - val_loss: 0.2176\n",
            "Epoch 151/200\n",
            "1179/1179 [==============================] - 1s 975us/step - loss: 0.0348 - val_loss: 0.2156\n",
            "Epoch 152/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0370 - val_loss: 0.2162\n",
            "Epoch 153/200\n",
            "1179/1179 [==============================] - 1s 991us/step - loss: 0.0323 - val_loss: 0.2153\n",
            "Epoch 154/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0347 - val_loss: 0.2227\n",
            "Epoch 155/200\n",
            "1179/1179 [==============================] - 1s 995us/step - loss: 0.0311 - val_loss: 0.2160\n",
            "Epoch 156/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0335 - val_loss: 0.2230\n",
            "Epoch 157/200\n",
            "1179/1179 [==============================] - 1s 987us/step - loss: 0.0351 - val_loss: 0.2131\n",
            "Epoch 158/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0335 - val_loss: 0.2146\n",
            "Epoch 159/200\n",
            "1179/1179 [==============================] - 1s 995us/step - loss: 0.0313 - val_loss: 0.2200\n",
            "Epoch 160/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0283 - val_loss: 0.2180\n",
            "Epoch 161/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0295 - val_loss: 0.2168\n",
            "Epoch 162/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0299 - val_loss: 0.2192\n",
            "Epoch 163/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0315 - val_loss: 0.2136\n",
            "Epoch 164/200\n",
            "1179/1179 [==============================] - 1s 988us/step - loss: 0.0300 - val_loss: 0.2127\n",
            "Epoch 165/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0328 - val_loss: 0.2143\n",
            "Epoch 166/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0310 - val_loss: 0.2115\n",
            "Epoch 167/200\n",
            "1179/1179 [==============================] - 1s 988us/step - loss: 0.0315 - val_loss: 0.2135\n",
            "Epoch 168/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0317 - val_loss: 0.2136\n",
            "Epoch 169/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0312 - val_loss: 0.2136\n",
            "Epoch 170/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0300 - val_loss: 0.2178\n",
            "Epoch 171/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0312 - val_loss: 0.2168\n",
            "Epoch 172/200\n",
            "1179/1179 [==============================] - 1s 999us/step - loss: 0.0309 - val_loss: 0.2172\n",
            "Epoch 173/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0324 - val_loss: 0.2191\n",
            "Epoch 174/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0285 - val_loss: 0.2138\n",
            "Epoch 175/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0299 - val_loss: 0.2136\n",
            "Epoch 176/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0322 - val_loss: 0.2167\n",
            "Epoch 177/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0309 - val_loss: 0.2190\n",
            "Epoch 178/200\n",
            "1179/1179 [==============================] - 1s 988us/step - loss: 0.0304 - val_loss: 0.2170\n",
            "Epoch 179/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0331 - val_loss: 0.2134\n",
            "Epoch 180/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0291 - val_loss: 0.2215\n",
            "Epoch 181/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0285 - val_loss: 0.2203\n",
            "Epoch 182/200\n",
            "1179/1179 [==============================] - 1s 992us/step - loss: 0.0303 - val_loss: 0.2155\n",
            "Epoch 183/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0292 - val_loss: 0.2180\n",
            "Epoch 184/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0322 - val_loss: 0.2129\n",
            "Epoch 185/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0308 - val_loss: 0.2145\n",
            "Epoch 186/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0309 - val_loss: 0.2207\n",
            "Epoch 187/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0301 - val_loss: 0.2176\n",
            "Epoch 188/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0311 - val_loss: 0.2139\n",
            "Epoch 189/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0273 - val_loss: 0.2205\n",
            "Epoch 190/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0291 - val_loss: 0.2130\n",
            "Epoch 191/200\n",
            "1179/1179 [==============================] - 1s 997us/step - loss: 0.0284 - val_loss: 0.2147\n",
            "Epoch 192/200\n",
            "1179/1179 [==============================] - 1s 995us/step - loss: 0.0259 - val_loss: 0.2196\n",
            "Epoch 193/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0278 - val_loss: 0.2168\n",
            "Epoch 194/200\n",
            "1179/1179 [==============================] - 1s 993us/step - loss: 0.0275 - val_loss: 0.2200\n",
            "Epoch 195/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0254 - val_loss: 0.2234\n",
            "Epoch 196/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0291 - val_loss: 0.2163\n",
            "Epoch 197/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0273 - val_loss: 0.2135\n",
            "Epoch 198/200\n",
            "1179/1179 [==============================] - 1s 1ms/step - loss: 0.0318 - val_loss: 0.2167\n",
            "Epoch 199/200\n",
            "1179/1179 [==============================] - 1s 996us/step - loss: 0.0278 - val_loss: 0.2132\n",
            "Epoch 200/200\n",
            "1179/1179 [==============================] - 1s 994us/step - loss: 0.0277 - val_loss: 0.2141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcbe634630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsrmWRYxbRCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_a, x_test_a, y_train_a, y_test_a = train_test_split(np.stack(list(df['image']), axis=0), list(df['arousal']), test_size=0.33)\n",
        "\n",
        "model_a.fit(x_train_a, y_train_a, epochs=100, batch_size=128, validation_data=(x_test_a, y_test_a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj3blHaFSQUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions on the testing data\n",
        "preds = model.predict(x_test)\n",
        " \n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - y_test\n",
        "percentDiff = (diff / y_test) * 100\n",
        "absPercentDiff = np.abs(percentDiff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUHJyLJXSM1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json_v = model_v.to_json()\n",
        "# Записываем модель в файл\n",
        "json_file = open(\"affect_model_v.json\", \"w\")\n",
        "json_file.write(model_json_v)\n",
        "json_file.close()\n",
        "\n",
        "model_v.save_weights(\"affect_model_v.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQjAoElpbb65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json_a = model_a.to_json()\n",
        "# Записываем модель в файл\n",
        "json_file = open(\"affect_model_a.json\", \"w\")\n",
        "json_file.write(model_json_a)\n",
        "json_file.close()\n",
        "\n",
        "model_a.save_weights(\"affect_model_a.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B775mvXQbm02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing import image\n",
        "\n",
        "print('start loading image...')\n",
        "img = image.load_img(\"hehehe.jpg\", target_size=(150, 150))\n",
        "print('start creating array from image...')\n",
        "x = image.img_to_array(img)\n",
        "print('start x /= 255...')\n",
        "x /= 255\n",
        "print('start expanding x...')\n",
        "x = np.expand_dims(x, axis=0)\n",
        "print('openning json...')\n",
        "json_file_v = open(\"affect_model_v.json\", \"r\")\n",
        "json_file_a = open(\"affect_model_a.json\", \"r\")\n",
        "print('reading json...')\n",
        "loaded_model_json_v = json_file_v.read()\n",
        "loaded_model_json_a = json_file_a.read()\n",
        "print('closing json...')\n",
        "json_file_v.close()\n",
        "json_file_a.close()\n",
        "print('creating model from json...')\n",
        "loaded_model_v = model_from_json(loaded_model_json_v)\n",
        "loaded_model_a = model_from_json(loaded_model_json_a)\n",
        "print('loading weights from cats_vgg19_model.h5...')\n",
        "loaded_model_v.load_weights(\"affect_model_v.h5\")\n",
        "loaded_model_a.load_weights(\"affect_model_a.h5\")\n",
        "print('compiling weights from cats_vgg19_model.h5...')\n",
        "loaded_model_v.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "loaded_model_a.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "print('getting model prediction from x...')\n",
        "prediction_v = loaded_model_v.predict(x)\n",
        "prediction_a = loaded_model_a.predict(x)\n",
        "print('creating array of breeds...')\n",
        "print(\"valence: \", prediction_v)\n",
        "print(\"arousal: \", prediction_a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}